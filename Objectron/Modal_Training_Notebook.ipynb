{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectron Bottle Training - Modal Environment\n",
    "Full 20-epoch training with stride-8 frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= \n",
    "# Cell 1 â€” System + Repo \n",
    "# ========================= \n",
    "\n",
    "# We're in Modal environment - current directory is already the workspace\n",
    "# Current path: /__modal/volumes/vo-c8JUdOcB4WvgoDoaQ9WUrQ/akshat.trivedi_flamapp.com\n",
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository if not already present\n",
    "import os\n",
    "if not os.path.exists('Objectron-bottle'):\n",
    "    !git clone https://github.com/akshattrivedi-flam/Objectron-bottle.git Objectron-bottle\n",
    "else:\n",
    "    print(\"Repository already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the Objectron directory\n",
    "%cd Objectron-bottle/Objectron\n",
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install opencv-python pillow tqdm matplotlib numpy scipy\n",
    "!pip install \"protobuf<=3.20.3\"  # For compatibility with generated protobuf code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Objectron Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/objectron', exist_ok=True)\n",
    "\n",
    "# Download bottle dataset\n",
    "def download_objectron_bottle():\n",
    "    \"\"\"Download Objectron bottle dataset\"\"\"\n",
    "    base_url = \"https://storage.googleapis.com/objectron\"
",
    "    \n",
    "    # Download train and test videos\n",
    "    for split in ['train', 'test']:\n",
    "        print(f\"Downloading {split} split...\")\n",
    "        \n",
    "        # Create split directory\n",
    "        split_dir = f'data/objectron/{split}'\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # Download bottle videos (we'll download a subset for now)\n",
    "        for batch_id in range(1, 6):  # 5 batches\n",
    "            video_url = f\"{base_url}/v1/bottle/{split}/batch-{batch_id}.tar.gz\"\n",
    "            tar_path = f\"{split_dir}/batch-{batch_id}.tar.gz\"\n",
    "            \n",
    "            if not os.path.exists(tar_path):\n",
    "                print(f\"Downloading batch {batch_id}...\")\n",
    "                try:\n",
    "                    subprocess.run(['wget', '-q', video_url, '-O', tar_path], check=True)\n",
    "                    # Extract\n",
    "                    subprocess.run(['tar', '-xzf', tar_path, '-C', split_dir], check=True)\n",
    "                    os.remove(tar_path)  # Remove tar file\n",
    "                except subprocess.CalledProcessError:\n",
    "                    print(f\"Failed to download batch {batch_id}\")\n",
    "                    continue\n",
    "\n",
    "download_objectron_bottle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data\n",
    "    stride = 8  # Extract every 8th frame\n",
    "    img_size = 224\n",
    "    \n",
    "    # Model\n",
    "    num_keypoints = 9  # 3D bounding box: 8 corners + center\n",
    "    \n",
    "    # Training\n",
    "    epochs = 20  # Full training without early stopping\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    # Hardware\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    num_workers = 6\n",
    "    \n",
    "    # Paths\n",
    "    data_root = 'data/objectron'\n",
    "    train_dir = 'data/objectron/train'\n",
    "    test_dir = 'data/objectron/test'\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    manifest_dir = 'manifests'\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(cfg.manifest_dir, exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame Extraction with Stride-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stride8_frames(video_path, output_dir, stride=8):\n",
    "    \"\"\"Extract frames with stride-8 from video\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_id = 0\n",
    "    extracted_frames = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Extract every stride-th frame\n",
    "        if frame_id % stride == 0:\n",
    "            frame_name = f\"frame_{frame_id:06d}.jpg\"\n",
    "            frame_path = os.path.join(output_dir, frame_name)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            extracted_frames.append(frame_path)\n",
    "        \n",
    "        frame_id += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return extracted_frames\n",
    "\n",
    "def process_objectron_videos():\n",
    "    \"\"\"Process all Objectron videos with stride-8 extraction\"\"\"\n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = f'data/objectron/{split}'\n",
    "        video_dirs = glob.glob(f\"{split_dir}/batch-*\")\n",
    "        \n",
    "        all_frames = []\n",
    "        \n",
    "        for video_dir in tqdm(video_dirs, desc=f\"Processing {split} videos\"):\n",
    "            video_files = glob.glob(f\"{video_dir}/*.mov\") + glob.glob(f\"{video_dir}/*.MP4\")\n",
    "            \n",
    "            for video_file in video_files:\n",
    "                # Create output directory for this video\n",
    "                video_name = Path(video_file).stem\n",
    "                output_dir = f\"{split_dir}/frames/{video_name}\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                \n",
    "                # Extract frames\n",
    "                frames = extract_stride8_frames(video_file, output_dir, cfg.stride)\n",
    "                all_frames.extend(frames)\n",
    "        \n",
    "        # Save manifest\n",
    "        manifest_path = f\"{cfg.manifest_dir}/{split}_manifest.pkl\"\n",
    "        with open(manifest_path, 'wb') as f:\n",
    "            pickle.dump(all_frames, f)\n",
    "        \n",
    "        print(f\"Extracted {len(all_frames)} frames for {split} split\")\n",
    "\n",
    "# Run frame extraction\n",
    "process_objectron_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectronDataset(Dataset):\n",
    "    def __init__(self, manifest_path, transform=None):\n",
    "        with open(manifest_path, 'rb') as f:\n",
    "            self.frame_paths = pickle.load(f)\n",
    "        \n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((cfg.img_size, cfg.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame_path = self.frame_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(frame_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # For now, create dummy keypoints (you'll need to implement actual keypoint loading)\n",
    "        # This is where you'd load your actual 3D bounding box keypoints\n",
    "        keypoints = torch.zeros(cfg.num_keypoints * 3)  # 9 keypoints * 3 coordinates\n",
    "        \n",
    "        return image, keypoints, frame_path\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ObjectronDataset(f\"{cfg.manifest_dir}/train_manifest.pkl\")\n",
    "test_dataset = ObjectronDataset(f\"{cfg.manifest_dir}/test_manifest.pkl\")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class MobileNetKeypointDetector(nn.Module):\n",
    "    def __init__(self, num_keypoints=9, pretrained=True):\n",
    "        super(MobileNetKeypointDetector, self).__init__()\n",
    "        \n",
    "        # Load pretrained MobileNetV2\n",
    "        self.backbone = models.mobilenet_v2(pretrained=pretrained)\n",
    "        \n",
    "        # Remove the final classifier\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # Get feature dimension\n",
    "        feature_dim = 1280\n",
    "        \n",
    "        # Keypoint regression head\n",
    "        self.keypoint_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_keypoints * 3)  # 9 keypoints * 3 coordinates\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Predict keypoints\n",
    "        keypoints = self.keypoint_head(features)\n",
    "        \n",
    "        return keypoints\n",
    "\n",
    "# Create model\n",
    "model = MobileNetKeypointDetector(num_keypoints=cfg.num_keypoints).to(cfg.device)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function with weighting for keypoint importance\n",
    "def weighted_mse_loss(pred, target, weights=None):\n",
    "    \"\"\"Weighted MSE loss for keypoint regression\"\"\"\n",
    "    if weights is None:\n",
    "        weights = torch.ones_like(pred)\n",
    "    \n",
    "    diff = pred - target\n",
    "    loss = (diff * diff) * weights\n",
    "    return loss.mean()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, \n",
    "                         num_workers=cfg.num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, \n",
    "                        num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with mixed precision\n",
    "def train_epoch(model, loader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, targets, _) in enumerate(tqdm(loader, desc=\"Training\")):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, _ in tqdm(loader, desc=\"Validation\"):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "import torch\n",
    "\n",
    "# Enable TF32 for A100 GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"TF32 enabled for A100 optimization\")\n",
    "\n",
    "# Initialize scaler for mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"Starting training for {cfg.epochs} epochs...\")\n",
    "print(f\"No early stopping - will train all epochs!\")\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, \n",
    "                           lambda pred, target: weighted_mse_loss(pred, target), \n",
    "                           cfg.device, scaler)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, test_loader, \n",
    "                         lambda pred, target: weighted_mse_loss(pred, target), \n",
    "                         cfg.device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    is_best = val_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses\n",
    "        }, f\"{cfg.checkpoint_dir}/best_model.pth\")\n",
    "        print(f\"Best model saved! Val loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Save checkpoint every epoch (for resumability)\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'scaler_state_dict': scaler.state_dict()\n",
    "    }, f\"{cfg.checkpoint_dir}/checkpoint_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "print(f\"\\nTraining completed! Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# LR plot\n",
    "plt.subplot(1, 2, 2)\n",
    "lrs = [cfg.learning_rate * (1 + np.cos(np.pi * epoch / cfg.epochs)) / 2 for epoch in range(cfg.epochs)]\n",
    "plt.plot(lrs, color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/training_progress.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': cfg,\n",
    "    'final_train_loss': train_losses[-1],\n",
    "    'final_val_loss': val_losses[-1],\n",
    "    'best_val_loss': best_val_loss\n",
    "}, f\"{cfg.checkpoint_dir}/final_model.pth\")\n",
    "\n",
    "print(\"Final model saved!\")\n",
    "print(f\"Model size: {os.path.getsize(f'{cfg.checkpoint_dir}/final_model.pth') / 1e6:.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}